# NLP Journey - Roadmap to Learn LLMs from Scratch with Modern NLP Methods in 2024

## Overview

This repository provides a comprehensive guide on leveraging Jupyter Notebooks for Natural Language Processing (NLP) tasks, culminating in the understanding and application of Large Language Models (LLMs). It focuses on the essential technical skills required for LLM and NLP-related jobs in 2024.
## NLP Journey - Roadmap to Learn LLMs from Scratch with Modern NLP Methods in 2024

## Overview

This repository provides a comprehensive guide on leveraging Jupyter Notebooks for Natural Language Processing (NLP) tasks, culminating in the understanding and application of Large Language Models (LLMs). It focuses on the essential technical skills required for LLM and NLP-related jobs in 2024.

## 1. Foundational NLP Skills

| Area                               | Topic                                    | Resources                                                                                                                                                                                                                                                                                               | Practices                                                                                                                                                                                                                                                                          |
|------------------------------------|-----------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Prerequisites: Python and Jupyter Notebooks** | **Setting up your environment** | [Python Installation Guide](link-to-python-guide), [Jupyter Notebook Installation Guide](link-to-jupyter-guide)                                                                                                                                                                                               | [Setting up your environment](link-to-setup-notebook)                                                                                                                                                                                                                                           |
| **Introduction to NLP** | **Core NLP Concepts (Syntax, Semantics, Pragmatics)** | [Stanford NLP Course](http://web.stanford.edu/class/cs224n/), [Speech and Language Processing (3rd ed. draft)](https://web.stanford.edu/~jurafsky/slp3/)                                                                                                                                                     | [Analyzing Text with Basic NLP Concepts](link-to-nlp-concepts-notebook)                                                                                                                                                                                                                                           |
| **Text Preprocessing** | **Tokenization, Stemming, Lemmatization** | [NLTK Book Chapter 3](https://www.nltk.org/book/ch03.html), [Text Preprocessing with NLTK](https://www.datacamp.com/tutorial/text-preprocessing-in-python-with-nltk)                                                                                                                                        | [Implementing Text Preprocessing Techniques](link-to-text-preprocessing-notebook)                                                                                                                                                                                                                                           |
| **Text Preprocessing** | **Stop Word Removal, Punctuation Removal** | [NLTK Stop Words](https://www.nltk.org/data.html), [Regular Expressions in Python](https://docs.python.org/3/library/re.html), [Removing Stop Words with NLTK in Python](https://stackabuse.com/removing-stop-words-from-strings-in-python/)                                                                        | [Implementing Stop Word and Punctuation Removal](link-to-stop-word-punctuation-removal-notebook)                                                                                                                                                                                                                                           |
| **Feature Engineering for NLP** | **Bag-of-Words (BoW) Model** | [Scikit-learn Bag-of-Words](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction), [Bag of Words Model in NLP](https://www.mygreatlearning.com/blog/bag-of-words/)                                                                                                            | [Implementing BoW Model](link-to-bow-notebook)                                                                                                                                                                                                                                           |
| **Feature Engineering for NLP** | **TF-IDF Model** | [Scikit-learn TF-IDF](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting), [TF-IDF from scratch in python](https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089)                                                 | [Implementing TF-IDF Model](link-to-tfidf-notebook)                                                                                                                                                                                                                                           |
| **Feature Engineering for NLP** | **N-grams** | [N-grams in Text Mining](https://en.wikipedia.org/wiki/N-gram), [Working with N-grams in Text](https://www.analyticsvidhya.com/blog/2021/09/what-are-n-grams-and-how-to-implement-them-in-python/)                                                                                                          | [Implementing N-grams for Text Analysis](link-to-ngrams-notebook)                                                                                                                                                                                                                                           |
| **Text Classification** | **Naive Bayes Classifier** | [Scikit-learn Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html), [Naive Bayes Classifier in NLP](https://www.analyticsvidhya.com/blog/2021/07/performing-sentiment-analysis-with-naive-bayes-classifier/)                                                                            | [Implementing Text Classification with Naive Bayes](link-to-naive-bayes-classification-notebook)                                                                                                                                                                                                                                           |
| **Text Classification** | **Support Vector Machines (SVM)** | [Scikit-learn SVM](https://scikit-learn.org/stable/modules/svm.html), [Support Vector Machine Algorithm in Machine Learning](https://www.analyticsvidhya.com/blog/2021/10/support-vector-machine-algorithm-in-machine-learning/)                                                                          | [Implementing Text Classification with SVM](link-to-svm-classification-notebook)                                                                                                                                                                                                                                           |
| **Text Classification** | **Logistic Regression** | [Scikit-learn Logistic Regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression), [Logistic Regression for Machine Learning](https://machinelearningmastery.com/logistic-regression-for-machine-learning/)                                                                      | [Implementing Text Classification with Logistic Regression](link-to-logistic-regression-classification-notebook)                                                                                                                                                                                                                                           |
| **Text Clustering** | **K-Means Clustering** | [Scikit-learn K-Means](https://scikit-learn.org/stable/modules/clustering.html#k-means), [K-Means Clustering in Machine Learning](https://www.analyticsvidhya.com/blog/2021/01/in-depth-intuition-of-k-means-clustering-algorithm-in-machine-learning/)                                                            | [Implementing Text Clustering with K-Means](link-to-kmeans-clustering-notebook)                                                                                                                                                                                                                                           |
| **Text Clustering** | **Hierarchical Clustering** | [Scikit-learn Hierarchical Clustering](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering), [Hierarchical Clustering in Machine Learning](https://www.analyticsvidhya.com/blog/2019/05/beginners-guide-hierarchical-clustering/)                                                       | [Implementing Text Clustering with Hierarchical Clustering](link-to-hierarchical-clustering-notebook)                                                                                                                                                                                                                                           |
| **Topic Modeling** | **Latent Dirichlet Allocation (LDA)** | [Gensim LDA](https://radimrehurek.com/gensim/models/ldamodel.html), [Topic Modeling with Gensim (Python)](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/)                                                                                                                      | [Implementing Topic Modeling with LDA](link-to-lda-topic-modeling-notebook)                                                                                                                                                                                                                                           |
| **Named Entity Recognition (NER)** | **Using NLTK and spaCy** | [NLTK NER](https://www.nltk.org/book/ch07.html), [spaCy NER](https://spacy.io/usage/linguistic-features#named-entities), [Named Entity Recognition (NER) using spaCy in Python](https://www.analyticsvidhya.com/blog/2021/06/named-entity-recognition-ner-using-spacy-in-python/)                                   | [Implementing NER with NLTK and spaCy](link-to-ner-notebook)                                                                                                                                                                                                                                           |
| **Sentiment Analysis** | **Lexicon-Based and Machine Learning Approaches** | [NLTK Sentiment Analysis](https://www.nltk.org/howto/sentiment.html), [Sentiment Analysis with Scikit-learn](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html), [Sentiment Analysis with Python NLTK Text Classification](https://realpython.com/sentiment-analysis-python/) | [Implementing Sentiment Analysis Techniques](link-to-sentiment-analysis-notebook)                                                                                                                                                                                                                                           |
| **Information Retrieval and Extraction** | **TF-IDF, BM25, and Query Expansion** | [Information Retrieval Textbook](https://nlp.stanford.edu/IR-book/html/htmledition/irbook.html), [TF-IDF and BM25](https://opensourceconnections.com/blog/2015/10/20/bm25-the-next-generation-of-tf-idf/), [Query Expansion Techniques](https://www.elastic.co/blog/practical-bm25-part-3-considerations-for-query-expansion)  | [Building a Simple Search Engine](link-to-search-engine-notebook)                                                                                                                                                                                                                                           |


## 2.  Core NLP with Deep Learning

| Area                       | Topic                                      | Resources                                                                                                                                                                                                                                                                                                           | Practices                                                                                                                                                                                                                                                                   |
|----------------------------|-------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Deep Learning Frameworks** | **TensorFlow Fundamentals: Tensors and Variables** | [TensorFlow Documentation](link-to-tensorflow-doc), [TensorFlow Tutorial for Beginners](https://www.tensorflow.org/tutorials)                                                                                                                                                                                                    | [TensorFlow Basics: Working with Tensors and Variables](link-to-notebook-3a)                                                                                                                                                                                                             |
| **Deep Learning Frameworks** | **TensorFlow Fundamentals: Building and Training a Simple NLP Model** | [TensorFlow Documentation](link-to-tensorflow-doc), [Build a text classification model](https://www.tensorflow.org/tutorials/keras/text_classification)                                                                                                                                                                     | [Building a Simple Text Classifier with TensorFlow](link-to-notebook-3b)                                                                                                                                                                                                           |
| **Deep Learning Frameworks** | **PyTorch Fundamentals: Tensors and Autograd** | [PyTorch Documentation](link-to-pytorch-doc), [PyTorch Tutorials](https://pytorch.org/tutorials/)                                                                                                                                                                                                                      | [PyTorch Basics: Working with Tensors and Autograd](link-to-notebook-3c)                                                                                                                                                                                                           |
| **Deep Learning Frameworks** | **PyTorch Fundamentals: Building and Training a Simple NLP Model** | [PyTorch Documentation](link-to-pytorch-doc), [NLP From Scratch: Classifying Names with a Character-Level RNN](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)                                                                                                                                | [Building a Simple Text Classifier with PyTorch](link-to-notebook-3d)                                                                                                                                                                                                             |
| **Neural Networks for NLP** | **Word Embeddings with Feedforward Networks** | [Word Embeddings Tutorial](link-to-word-embeddings-tutorial), [Word Embeddings in NLP](https://www.analyticsvidhya.com/blog/2020/03/pretrained-word-embeddings-nlp/)                                                                                                                                              | [Implementing Word Embeddings with a Feedforward Network](link-to-notebook-2a)                                                                                                                                                                                                     |
| **Neural Networks for NLP** | **Recurrent Neural Networks (RNNs) for Sequence Modeling** | [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/), [Recurrent Neural Networks Tutorial, Part 1 â€“ Introduction to RNNs](https://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)                                                           | [Implementing Text Classification with RNNs](link-to-notebook-2b)                                                                                                                                                                                                           |
| **Neural Networks for NLP** | **Long Short-Term Memory (LSTM) Networks** | [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/), [Illustrated Guide to LSTM's and GRU's: A step by step explanation](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)                                                     | [Implementing Sequence Modeling with LSTM Networks](link-to-lstm-notebook)                                                                                                                                                                                                     |
| **Neural Networks for NLP** | **Gated Recurrent Unit (GRU) Networks** | [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](https://arxiv.org/abs/1412.3555), [GRU Neural Networks: A Practical Guide with Examples in Python](https://machinelearningmastery.com/gru-neural-networks-a-practical-guide-with-examples-in-python/)                                       | [Implementing Sequence Modeling with GRU Networks](link-to-gru-notebook)                                                                                                                                                                                                     |
| **CNNs for NLP**          | **Text Classification with Convolutional Neural Networks (CNNs)** | [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882), [Understanding Convolutional Neural Networks for NLP](https://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)                                                                        | [Implementing Text Classification with CNNs](link-to-notebook-2c)                                                                                                                                                                                                             |
| **CNNs for NLP**          | **Sequence Labeling with CNNs** | [A Convolutional Neural Network for Modelling Sentences](https://arxiv.org/abs/1404.2188), [Sequence Labeling with CNNs](https://towardsdatascience.com/sequence-labeling-with-convolutional-neural-networks-cnn-e9039ab43563)                                                                                                | [Implementing Sequence Labeling with CNNs](link-to-notebook-2d)                                                                                                                                                                                                             |
| **Word Embeddings**       | **Word2Vec (Skip-gram and CBOW)** | [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781), [Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/), [Word2Vec Tutorial - The CBOW Model](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/) | [Implementing Word2Vec](link-to-word2vec-notebook)                                                                                                                                                                                                     |
| **Word Embeddings**       | **GloVe (Global Vectors for Word Representation)** | [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/), [GloVe Explained: Understanding GloVe Word Embeddings](https://neptune.ai/blog/glove-embeddings-guide)                                                                                                                              | [Implementing GloVe](link-to-glove-notebook)                                                                                                                                                                                                     |
| **Word Embeddings**       | **FastText** | [Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606), [FastText Tutorial: Better Word Embeddings with Subword Information](https://www.analyticsvidhya.com/blog/2021/07/understanding-fasttext-a-powerful-library-for-text-classification-and-representation/)                              | [Implementing FastText](link-to-fasttext-notebook)                                                                                                                                                                                                     |
| **Attention Mechanisms**   | **Self-Attention** | [Attention Is All You Need](https://arxiv.org/abs/1706.03762), [Illustrated Self-Attention](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)                                                                                                                                                  | [Implementing Self-Attention](link-to-self-attention-notebook)                                                                                                                                                                                                     |
| **Attention Mechanisms**   | **Multi-Head Attention** | [Attention Is All You Need](https://arxiv.org/abs/1706.03762), [Multi-Head Attention Explained](https://towardsdatascience.com/multi-head-attention-explained-f8442137433f)                                                                                                                                          | [Implementing Multi-Head Attention](link-to-multi-head-attention-notebook)                                                                                                                                                                                                     |
| **Transformers**           | **Transformer Architecture (Encoder and Decoder)** | [Attention Is All You Need](https://arxiv.org/abs/1706.03762), [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/), [Transformers in NLP: A Beginner's Guide](https://www.analyticsvidhya.com/blog/2021/06/transformers-nlp-a-beginners-guide/)                                                    | [Implementing a Transformer](link-to-transformer-notebook)                                                                                                                                                                                                     |
| **Contextualized Word Embeddings** | **ELMo (Embeddings from Language Models)** | [Deep contextualized word representations](https://arxiv.org/abs/1802.05365), [ELMo: Deep contextualized word representations](https://allennlp.org/elmo)                                                                                                                                                             | [Implementing ELMo](link-to-elmo-notebook)                                                                                                                                                                                                     |
| **Contextualized Word Embeddings** | **BERT (Bidirectional Encoder Representations from Transformers)** | [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805), [BERT Explained: State of the art language model for NLP](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)                                        | [Implementing BERT](link-to-bert-notebook)                                                                                                                                                                                                     |
| **Sequence-to-Sequence Models** | **Seq2Seq Models with Attention** | [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473), [Seq2Seq with Attention and Beam Search](https://machinelearningmastery.com/encoder-decoder-long-short-term-memory-networks/)                                                                                     | [Implementing Seq2Seq with Attention](link-to-seq2seq-attention-notebook)                                                                                                                                                                                                     |
| **Machine Translation** | **Neural Machine Translation (NMT) with Transformers** | [Attention Is All You Need](https://arxiv.org/abs/1706.03762), [Neural Machine Translation with Attention](https://www.tensorflow.org/tutorials/text/nmt_with_attention)                                                                                                                                            | [Implementing NMT with Transformers](link-to-nmt-transformer-notebook)                                                                                                                                                                                                     |
| **Speech Recognition and Transcription** | **Automatic Speech Recognition (ASR) with Deep Learning** | [Automatic Speech Recognition: A Deep Learning Approach](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/38131.pdf), [Deep Speech: Scaling up end-to-end speech recognition](https://arxiv.org/abs/1412.5567)                                                                              | [Implementing ASR with Deep Learning](link-to-asr-deep-learning-notebook)                                                                                                                                                                                                     |


## 3.  Working with Large Language Models (LLMs)

| Area           | Topic                                                           | Resources                                                                                                                                                                                                                                                                                                                                                                      | Practices                                                                                                                                                                                                                                                                          |
|----------------|-----------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Fundamentals of LLMs** | **Transformer Architecture Deep Dive**                         | [The Illustrated Transformer](link-to-illustrated-transformer), [Transformer: A Novel Neural Network Architecture for Language Understanding](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html), **"Attention Is All You Need"** [Paper](https://arxiv.org/abs/1706.03762)                                                                                 | [Analyzing the Transformer Architecture in Detail](link-to-transformer-deep-dive-notebook)                                                                                                                                                                                                                        |
| **Fundamentals of LLMs** | **Exploring Different Architectures (GPT, BERT, T5, etc.)**       | [Hugging Face Transformer Models](link-to-huggingface-transformer-models), **"Language Models are Few-Shot Learners"** [Paper](https://arxiv.org/abs/2005.14165), **"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** [Paper](https://arxiv.org/abs/1810.04805), **"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"** [Paper](https://arxiv.org/abs/1910.10683) | [Comparing and Contrasting Different LLM Architectures](link-to-llm-architectures-comparison-notebook)                                                                                                                                                                                                                  |
| **Fundamentals of LLMs: Key Concepts** | **Pre-training Objectives (Masked Language Modeling, Causal Language Modeling)** | **"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** [Paper](https://arxiv.org/abs/1810.04805), **"Improving Language Understanding by Generative Pre-Training"** [Paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf), [A Comprehensive Guide to Pretraining](https://huggingface.co/blog/pretraining) | [Implementing Different Pre-training Objectives](link-to-llm-pretraining-objectives-notebook)                                                                                                                                                                                                                       |
| **Fundamentals of LLMs: Key Concepts** | **Fine-tuning Techniques for Downstream Tasks**                 | [Fine-tuning Language Models for Downstream Tasks](link-to-llm-finetuning-guide), **"How to Fine-Tune BERT for Text Classification?"** [Blog](https://mccormickml.com/2019/07/29/bert-fine-tuning/), [Fine-tuning a pretrained model](https://huggingface.co/docs/transformers/training)                                                                                                 | [Fine-tuning an LLM for a Specific Task](link-to-llm-finetuning-techniques-notebook)                                                                                                                                                                                                                               |
| **Fundamentals of LLMs: Key Concepts** | **Prompt Engineering Strategies for Optimal Performance**        | [Prompt Engineering Guide](link-to-prompt-engineering-guide), **"The Power of Scale for Parameter-Efficient Prompt Tuning"** [Paper](https://arxiv.org/abs/2104.08691), [Prompt Engineering](https://www.promptingguide.ai/)                                                                                                                                     | [Developing Effective Prompt Engineering Techniques](link-to-llm-prompt-engineering-strategies-notebook)                                                                                                                                                                                                              |
| **Fine-tuning LLMs** | **Text Classification with Pre-trained LLMs**                   | [Fine-tuning for Text Classification with Hugging Face](link-to-huggingface-text-classification-finetuning), **"BERT for Text Classification with TensorFlow"** [Tutorial](https://www.tensorflow.org/tutorials/text/classify_text_with_bert), [Fine-tuning with custom datasets](https://huggingface.co/docs/transformers/training#fine-tuning-with-custom-datasets)                           | [Fine-tuning a Pre-trained LLM for Text Classification](link-to-notebook-10f)                                                                                                                                                                                                                                   |
| **Fine-tuning LLMs** | **Question Answering with Pre-trained LLMs**                    | [Fine-tuning for Question Answering with Hugging Face](link-to-huggingface-question-answering-finetuning), **"BERT for Question Answering with TensorFlow"** [Tutorial](https://www.tensorflow.org/tutorials/text/classify_text_with_bert), [Question Answering](https://huggingface.co/docs/transformers/tasks/question-answering)                                                           | [Fine-tuning a Pre-trained LLM for Question Answering](link-to-notebook-10g)                                                                                                                                                                                                                                   |
| **Fine-tuning LLMs: Parameter-Efficient Fine-Tuning (PEFT)** | **LoRA (Low-Rank Adaptation): Theory and Implementation**         | **"LoRA: Low-Rank Adaptation of Large Language Models"** [Paper](link-to-lora-paper), **"Understanding LoRA"** [Blog](https://huggingface.co/blog/lora), [Parameter-Efficient Fine-Tuning (PEFT)](https://huggingface.co/docs/peft/index)                                                                                                                | [Implementing LoRA for Fine-tuning an LLM](link-to-notebook-10i)                                                                                                                                                                                                                                                   |
| **Fine-tuning LLMs: Parameter-Efficient Fine-Tuning (PEFT)** | **Adapter Modules: Theory and Implementation**                  | **"Adapter Modules for Parameter-Efficient Fine-tuning of Large Language Models"** [Paper](link-to-adapter-modules-paper), **"AdapterHub: A Framework for Adapting Transformers"** [Website](https://adapterhub.ml/), [Parameter-Efficient Fine-Tuning (PEFT)](https://huggingface.co/docs/peft/index)                                                                                   | [Implementing Adapter Modules for Fine-tuning an LLM](link-to-notebook-10j)                                                                                                                                                                                                                                           |
| **Chatbot Development with LLMs** | **Building Conversational AI with GPT-3 and Other LLMs**        | [OpenAI GPT-3 Documentation](link-to-gpt-3-doc), **"Building a Chatbot with the OpenAI API"** [Doc](https://beta.openai.com/docs/guides/chat), [Chat with a Model](https://platform.openai.com/playground?mode=chat)                                                                                                                                                      | [Building a Chatbot with a Pre-trained LLM](link-to-llm-chatbot-notebook)                                                                                                                                                                                                              |
| **Content Moderation using LLMs** | **Detecting and Filtering Harmful Content with LLMs**           | [Content Moderation with LLMs](link-to-content-moderation-llms-tutorial), [Perspective API](https://www.perspectiveapi.com/), [Content Moderation](https://huggingface.co/docs/transformers/tasks/content-moderation)                                                                                                                                              | [Building a Content Moderation System with an LLM](link-to-llm-content-moderation-notebook)                                                                                                                                                                                                              |
| **Automated Writing Assistance Tools** | **Grammar and Style Checking with LLMs**                      | [Grammarly API Documentation](link-to-grammarly-api-doc), [LanguageTool API](https://languagetool.org/http-api/), [Grammar Correction](https://huggingface.co/docs/transformers/tasks/grammar-correction)                                                                                                                                                          | [Building a Grammar and Style Checker with an LLM](link-to-llm-grammar-style-checker-notebook)                                                                                                                                                                                                              |
| **Text Generation for Content Creation** | **Generating Articles and Social Media Posts with LLMs**         | [Copy.ai API Documentation](link-to-copy-ai-api-doc), [ShortlyAI API](https://shortlyai.com/api/), [Text Generation](https://huggingface.co/docs/transformers/tasks/text-generation)                                                                                                                                                                          | [Building a Content Generator with an LLM](link-to-llm-content-generator-notebook)                                                                                                                                                                                                              |
| **Predictive Analytics using NLP** | **Predicting Trends with Text Data and LLMs**                   | [Predictive Analytics with NLP](link-to-predictive-analytics-nlp-tutorial), **"Using LLMs for Time Series Forecasting"** [Blog](https://towardsdatascience.com/using-llms-for-time-series-forecasting-a-comprehensive-guide-7fa51121411c), [Time Series Forecasting](https://huggingface.co/docs/transformers/tasks/time-series-forecasting)                                                  | [Building a Predictive Model with Text Data and an LLM](link-to-llm-predictive-analytics-notebook)                                                                                                                                                                                                              |
| **Dialogue Management for Conversational Interfaces** | **Building Advanced Chatbots with Rasa and Dialogflow**         | [Rasa Documentation](link-to-rasa-doc), [Dialogflow Documentation](link-to-dialogflow-doc), [Rasa and LLMs](https://rasa.com/blog/how-to-build-a-gpt-powered-chatbot-with-rasa/), [Dialogflow and LLMs](https://cloud.google.com/dialogflow/es/docs/integrations/agents/generative-ai)                                                     | [Building a Contextual Chatbot with Rasa and an LLM](link-to-rasa-llm-chatbot-notebook), [Building a Chatbot with Dialogflow and an LLM](link-to-dialogflow-llm-chatbot-notebook)                                                                                                                  |
| **Working with Long Context in LLMs** | **Understanding and Addressing Challenges of Long Context** | **"A Survey on Efficient Inference for Large Language Models"** [Paper](https://arxiv.org/abs/2307.02230), **"The CAP Principle for LLM Serving: A Survey of Long-Context Large Language Model Serving"** [Paper](https://arxiv.org/abs/2310.17658), [Long Context Language Modeling](https://huggingface.co/blog/long-context-language-modeling), **"LongFin: A Multimodal Document Understanding Model for Long Financial Domain Documents"** [Paper](https://aclanthology.org/2023.findings-acl.307.pdf), **"DocFinQA: A Long-Context Financial Reasoning Dataset"** [Paper](https://aclanthology.org/2023.findings-acl.306.pdf) | [Exploring Techniques for Handling Long Context in LLMs](link-to-long-context-llm-notebook)                                                                                                                                                                                                     |


## 4.  Building Advanced NLP Applications

| Area                               | Topic                                                                  | Resources                                                                                                                                                                                                                                                                                                                                                                                  | Practices                                                                                                                                                                                                                                                                          |
|------------------------------------|-----------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Advanced Text Classification** | **Ensemble Methods: Bagging, Boosting (AdaBoost, Gradient Boosting)** | [Ensemble Methods for Machine Learning](link-to-ensemble-methods-tutorial), [Scikit-learn Ensemble Methods](https://scikit-learn.org/stable/modules/ensemble.html), [Ensemble Methods in Machine Learning: What are They and Why Use Them?](https://towardsdatascience.com/ensemble-methods-in-machine-learning-what-are-they-and-why-use-them-68ec3f9fef5f)                                                        | [Implementing Ensemble Methods for Text Classification](link-to-ensemble-methods-text-classification-notebook)                                                                                                                                                                                |
| **Advanced Text Classification** | **Transfer Learning with Pre-trained Language Models (PLMs): Fine-tuning BERT, RoBERTa, XLNet** | [Hugging Face Transfer Learning Tutorial](link-to-huggingface-transfer-learning-tutorial), **"RoBERTa: A Robustly Optimized BERT Pretraining Approach"** [Paper](https://arxiv.org/abs/1907.11692), **"XLNet: Generalized Autoregressive Pretraining for Language Understanding"** [Paper](https://arxiv.org/abs/1906.08237), [Fine-tuning with custom datasets](https://huggingface.co/docs/transformers/training#fine-tuning-with-custom-datasets) | [Fine-tuning Different PLMs for Text Classification](link-to-fine-tuning-plms-text-classification-notebook)                                                                                                                                                                               |
| **Self-Supervised Learning**      | **Contrastive Learning for NLP: SimCSE, ConSERT**                   | **"SimCSE: Simple Contrastive Learning of Sentence Embeddings"** [Paper](https://arxiv.org/abs/2104.08821), **"ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer"** [Paper](https://arxiv.org/abs/2105.11741), [Contrastive Learning in NLP](https://lilianweng.github.io/posts/2021-05-31-contrastive/)                                                                      | [Implementing Contrastive Learning Methods for NLP](link-to-contrastive-learning-nlp-notebook)                                                                                                                                                                                |
| **Self-Supervised Learning**      | **Masked Language Modeling (MLM): BERT, RoBERTa, ALBERT**           | **"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** [Paper](https://arxiv.org/abs/1810.04805), **"RoBERTa: A Robustly Optimized BERT Pretraining Approach"** [Paper](https://arxiv.org/abs/1907.11692), **"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"** [Paper](https://arxiv.org/abs/1909.11942), [Masked Language Modeling](https://huggingface.co/docs/transformers/tasks/masked-language-modeling) | [Implementing Masked Language Modeling with Different Architectures](link-to-masked-language-modeling-notebook)                                                                                                                                                                                |
| **Question Answering and Reading Comprehension** | **Extractive Question Answering: BERT, XLNet, SpanBERT**              | **"BERT for Question Answering"** [Paper](https://arxiv.org/abs/1810.04805), **"XLNet for Question Answering"** [Paper](https://arxiv.org/abs/1906.08237), **"SpanBERT: Improving Pre-training by Representing and Predicting Spans"** [Paper](https://arxiv.org/abs/1907.10529), [Extractive Question Answering](https://huggingface.co/docs/transformers/tasks/question-answering)                             | [Implementing Extractive Question Answering with Different Architectures](link-to-extractive-qa-notebook)                                                                                                                                                                                |
| **Question Answering and Reading Comprehension** | **Abstractive Question Answering: BART, T5**                        | **"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"** [Paper](https://arxiv.org/abs/1910.13461), **"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"** [Paper](https://arxiv.org/abs/1910.10683), [Abstractive Question Answering](https://huggingface.co/docs/transformers/en/tasks/question-answering) | [Implementing Abstractive Question Answering with Different Architectures](link-to-abstractive-qa-notebook)                                                                                                                                                                                |
| **Dialogue Systems and Chatbots**  | **Rule-Based Chatbots: Designing Conversation Flows and Rules**        | [Building a Rule-Based Chatbot with Rasa](https://rasa.com/docs/rasa/tutorial/rule-based-chatbots/), [Building a Simple Chatbot from Scratch in Python (using NLTK)](https://medium.com/analytics-vidhya/building-a-simple-chatbot-in-python-using-nltk-7c8c8215ac6e)                                                                                                                            | [Implementing a Rule-Based Chatbot with Custom Rules](link-to-rule-based-chatbot-notebook)                                                                                                                                                                                |
| **Dialogue Systems and Chatbots**  | **Retrieval-Based Chatbots: Using Sentence Embeddings and Similarity Search** | [Retrieval-Based Chatbots with Elasticsearch and Sentence Transformers](link-to-retrieval-based-chatbot-tutorial), **"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"** [Paper](https://arxiv.org/abs/1908.10084), [Building a QA Chatbot with Sentence Transformers](https://www.sbert.net/examples/applications/semantic-search/README.html)                                                                  | [Implementing a Retrieval-Based Chatbot with Sentence Embeddings](link-to-retrieval-based-chatbot-notebook)                                                                                                                                                                                |
| **Dialogue Systems and Chatbots**  | **Generative Chatbots: Fine-tuning LLMs for Conversational AI**       | [Building a Generative Chatbot with TensorFlow](link-to-generative-chatbot-tutorial), **"DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation"** [Paper](https://arxiv.org/abs/1911.00536), [Building a Chatbot with Transformers](https://huggingface.co/blog/how-to-build-a-chatbot-with-transformers)                                                                        | [Implementing a Generative Chatbot with a Fine-tuned LLM](link-to-generative-chatbot-notebook)                                                                                                                                                                                |
| **Summarization Techniques**      | **Extractive Summarization: TextRank, LexRank**                     | **"TextRank: Bringing Order into Texts"** [Paper](https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf), **"LexRank: Graph-based Lexicographic Ranking for Sentence Extraction"** [Paper](https://www.aclweb.org/anthology/W04-3252.pdf), [Extractive Text Summarization Using spaCy in Python](https://www.analyticsvidhya.com/blog/2020/03/spacy-tutorial-learn-natural-language-processing-python/) | [Implementing Extractive Summarization Techniques](link-to-extractive-summarization-notebook)                                                                                                                                                                                |
| **Summarization Techniques**      | **Abstractive Summarization: Pointer-Generator Networks, Transformer-based Models** | **"Get To The Point: Summarization with Pointer-Generator Networks"** [Paper](https://arxiv.org/abs/1704.04368), [Fine-tune BART for Abstractive Summarization](https://huggingface.co/facebook/bart-large-cnn), [Abstractive Summarization with Transformers](https://huggingface.co/docs/transformers/en/tasks/summarization)                                                                                 | [Implementing Abstractive Summarization Techniques](link-to-abstractive-summarization-notebook)                                                                                                                                                                                |
| **Syntactic Parsing**             | **Dependency Parsing: Transition-based Parsing, Graph-based Parsing** | [Dependency Parsing](https://en.wikipedia.org/wiki/Dependency_parsing), [spaCy Dependency Parsing](https://spacy.io/usage/linguistic-features#dependency-parse), [Dependency Parsing Tutorial](https://nlp.stanford.edu/software/nndep.shtml)                                                                                                                                         | [Implementing Dependency Parsing Techniques](link-to-dependency-parsing-notebook)                                                                                                                                                                                |
| **Syntactic Parsing**             | **Constituency Parsing: CKY Algorithm, Chart Parsing**            | [Constituency Parsing](https://en.wikipedia.org/wiki/Constituency_grammar), [NLTK Constituency Parsing](https://www.nltk.org/book/ch08.html), [Constituency Parsing](https://www.nltk.org/_modules/nltk/parse/chart.html)                                                                                                                                                                            | [Implementing Constituency Parsing Techniques](link-to-constituency-parsing-notebook)                                                                                                                                                                                |
| **LLMs on Tabular Data**          | **Classification: Adapting LLMs for Tabular Data**                  | [Using LLMs with Tabular Data](link-to-llms-tabular-data-tutorial), **"TAPAS: Weakly Supervised Table Parsing via Pre-training"** [Paper](https://arxiv.org/abs/2004.02349), [Using LLMs on Tabular Data](https://towardsdatascience.com/using-llms-on-tabular-data-a-practical-guide-with-examples-in-python-b7dcd4036f64)                                                                   | [Implementing LLMs for Tabular Data Classification](link-to-llm-tabular-classification-notebook)                                                                                                                                                                                |
| **LLMs on Tabular Data**          | **Generation: Generating Text from Tabular Data with LLMs**         | [Using LLMs with Tabular Data](link-to-llms-tabular-data-tutorial), **"Table-to-Text Generation with LLMs"** [Paper](https://arxiv.org/abs/2202.06816), [Tabular Data to Text with Transformers](https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.pipeline.Text2TextGenerationPipeline)                                                                                          | [Implementing LLMs for Tabular Data Generation](link-to-llm-tabular-generation-notebook)                                                                                                                                                                                |
| **Enhanced Model Architectures**  | **Sparse Expert Models (Mixture-of-Experts - MoE): Scaling LLMs**     | [Mixture-of-Experts (MoE)](link-to-moe-tutorial), **"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"** [Paper](https://arxiv.org/abs/2006.16668), [Mixture-of-Experts with Expert Choice Routing](https://huggingface.co/blog/moe)                                                                                                                         | [Implementing MoE Models for NLP](link-to-moe-nlp-notebook)                                                                                                                                                                                |
| **Enhanced Model Architectures**  | **Retrieval Augmented Generation (RAG): Combining LLMs with Knowledge Bases** | [Retrieval Augmented Generation (RAG)](link-to-rag-tutorial), **"REALM: Retrieval-Augmented Language Model Pre-Training"** [Paper](https://arxiv.org/abs/2002.08909), **"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"** [Paper](https://arxiv.org/abs/2005.11401), [Retrieval-Augmented Generation (RAG)](https://huggingface.co/docs/transformers/en/model_doc/rag)           | [Implementing RAG for Knowledge-Intensive NLP Tasks](link-to-rag-nlp-notebook)                                                                                                                                                                                |
| **Enhanced Model Architectures**  | **Reinforcement Learning from Human Feedback (RLHF): Aligning LLMs with Human Preferences** | [Reinforcement Learning from Human Feedback (RLHF)](link-to-rlhf-tutorial), **"Training Language Models to Follow Instructions with Human Feedback"** [Paper](https://arxiv.org/abs/2203.02155), [Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf)                                                                                                 | [Implementing RLHF for Fine-tuning LLMs](link-to-rlhf-llm-notebook)                                                                                                                                                                                |
| **LLM Safety and Ethics**          | **Identifying and Analyzing Bias in LLMs: Tools and Techniques**     | **"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?"** [Paper](link-to-stochastic-parrots-paper), **"StereoSet: Measuring stereotypical bias in pretrained language models"** [Paper](https://arxiv.org/abs/2004.09456), [Evaluating and Mitigating Bias in Language Models](https://developers.google.com/machine-learning/fairness-overview/evaluate-mitigate-bias)                                   | [Identifying and Analyzing Bias in LLMs](link-to-llm-bias-analysis-notebook)                                                                                                                                                                                |
| **LLM Safety and Ethics**          | **Implementing Bias Mitigation Techniques: Data Augmentation, Debiasing Algorithms** | [Mitigating Bias in Language Models](link-to-bias-mitigation-tutorial), **"Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem"** [Paper](https://arxiv.org/abs/1808.09176), [Fairness Indicators](https://www.tensorflow.org/tfx/guide/fairness_indicators)                                                                                                                     | [Implementing Bias Mitigation Techniques for LLMs](link-to-llm-bias-mitigation-notebook)                                                                                                                                                                                |
| **LLM Safety and Ethics**          | **Developing Guidelines for Responsible LLM Use and Deployment**      | [Responsible AI Principles](link-to-responsible-ai-principles), **"On the Opportunities and Risks of Foundation Models"** [Paper](https://arxiv.org/abs/2108.07258), [Responsible Use of Large Language Models](https://ai.googleblog.com/2023/04/responsible-use-of-large-language.html)                                                                                                    | [Developing a Responsible LLM Use Policy](link-to-responsible-llm-use-policy-notebook)                                                                                                                                                                                |
| **Emerging Trends in LLMs**        | **In-Context Learning: Adapting LLMs without Fine-tuning**         | [In-Context Learning for Large Language Models: A Survey](link-to-in-context-learning-survey), **"Language Models are Few-Shot Learners"** [Paper](https://arxiv.org/abs/2005.14165), [In-Context Learning](https://huggingface.co/blog/in-context-learning)                                                                                                                                      | [Experimenting with In-Context Learning in LLMs](link-to-in-context-learning-llm-notebook)                                                                                                                                                                                |
| **Emerging Trends in LLMs**        | **Chain-of-Thought Prompting: Eliciting Reasoning in LLMs**          | **"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"** [Paper](link-to-chain-of-thought-prompting-paper), **"Large Language Models are Zero-Shot Reasoners"** [Paper](https://arxiv.org/abs/2205.11916), [Chain-of-Thought Prompting](https://huggingface.co/blog/chain-of-thought-prompting)                                                                                                   | [Experimenting with Chain-of-Thought Prompting in LLMs](link-to-chain-of-thought-prompting-llm-notebook)                                                                                                                                                                                |


## 5.  Developing Multimodal NLP Systems

| Area                               | Topic                                                          | Resources                                                                                                                                                                                                                                                                                                                                                                                               | Practices                                                                                                                                                                                                                                                                   |
|------------------------------------|---------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Introduction to Multi-modal LLMs** | **Architectures and Capabilities of Multimodal Models**          | [Multimodal Deep Learning](link-to-multimodal-deep-learning-overview), **"VL-BERT: Pre-training of Generic Visual-Linguistic Representations"** [Paper](https://arxiv.org/abs/1908.08530), [Multimodal Machine Learning: A Survey and Taxonomy](https://arxiv.org/abs/1705.09406)                                                                                                                                                 | [Exploring Different Multi-modal Architectures](link-to-notebook-11a)                                                                                                                                                                                                                                     |
| **Introduction to Multi-modal LLMs** | **Vision-Language Models (VLMs): Understanding and Applications** | [Vision-Language Pre-training for Image Captioning and Question Answering](link-to-vlm-paper), **"CLIP: Connecting Text and Images"** [Blog](https://openai.com/blog/clip/), [Vision-and-Language: Models and Applications](https://lilianweng.github.io/posts/2023-01-28-vlm/)                                                                                                                                       | [Understanding the Capabilities of Vision-Language Models](link-to-notebook-11b)                                                                                                                                                                                                                          |
| **Integrating Text and Image Data** | **Early Fusion Techniques: Concatenation, Feature Extraction**  | [Early Fusion for Image and Text Classification](link-to-early-fusion-tutorial), **"Multimodal Fusion Techniques for Sentiment Analysis"** [Paper](https://www.researchgate.net/publication/344006573_Multimodal_Fusion_Techniques_for_Sentiment_Analysis), [Early Fusion for Image and Text Classification](https://towardsdatascience.com/multimodal-sentiment-analysis-using-early-fusion-with-tensorflow-2-0-651c74508b45) | [Implementing Early Fusion for Text and Image Data](link-to-notebook-11c)                                                                                                                                                                                                                                     |
| **Integrating Text and Image Data** | **Late Fusion Techniques: Ensemble Methods, Decision Fusion**     | [Late Fusion for Image and Text Classification](link-to-late-fusion-tutorial), **"Multimodal Fusion Techniques for Sentiment Analysis"** [Paper](https://www.researchgate.net/publication/344006573_Multimodal_Fusion_Techniques_for_Sentiment_Analysis), [Late Fusion for Multimodal Sentiment Analysis](https://towardsdatascience.com/multimodal-sentiment-analysis-using-late-fusion-with-tensorflow-2-0-a669123303d1)     | [Implementing Late Fusion for Text and Image Data](link-to-notebook-11d)                                                                                                                                                                                                                                     |
| **Integrating Text and Audio Data** | **Feature Extraction: MFCCs, Spectrograms**                     | **"Speech Processing for Machine Learning: Filter banks, Mel-Frequency Cepstral Coefficients (MFCCs) and What's In-Between"** [Blog](https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html), [Audio Feature Extraction with Librosa](https://librosa.org/doc/latest/feature.html)                                                                                                                          | [Implementing Feature Extraction for Text and Audio Data](link-to-notebook-11e)                                                                                                                                                                                                                         |
| **Integrating Text and Audio Data** | **Attention Mechanisms for Multimodal Fusion**                 | **"Multimodal Attention Networks for Sentiment Analysis"** [Paper](https://arxiv.org/abs/1804.07471), [Multimodal Fusion with Attention Mechanism](https://towardsdatascience.com/multimodal-fusion-with-attention-mechanism-for-sentiment-analysis-9b80364bb491)                                                                                                                                 | [Implementing Attention Mechanisms for Multimodal Fusion](link-to-notebook-11f)                                                                                                                                                                                                                         |
| **Evaluating Multi-modal Systems**  | **Understanding and Implementing Metrics (Accuracy, F1-Score, BLEU, METEOR)** | [Evaluation Metrics for Multimodal Tasks](link-to-multimodal-evaluation-metrics), **"BLEU: a Method for Automatic Evaluation of Machine Translation"** [Paper](https://www.aclweb.org/anthology/P02-1040.pdf), [METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments](https://www.aclweb.org/anthology/W05-0909/)                                                     | [Implementing and Evaluating Multi-modal Metrics](link-to-notebook-11h)                                                                                                                                                                                                                                     |
| **Advancements in Multi-modal Capabilities** | **Vision-Language Models (VLMs) and Their Applications: Image Captioning, Visual Question Answering, Visual Reasoning** | [Recent Advances in Vision-Language Models](link-to-vlm-advances), **"VisualBERT: A Simple and Performant Baseline for Vision and Language"** [Paper](https://arxiv.org/abs/1908.03557), [Image Captioning and Visual Question Answering](https://huggingface.co/docs/transformers/en/tasks/image_captioning)                                                                                             | [Experimenting with Advanced VLM Capabilities](link-to-notebook-11j)                                                                                                                                                                                                                                     |
| **Advancements in Multi-modal Capabilities** | **Image Captioning with VLMs: Show, Attend, and Tell, Transformer-based Models** | **"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"** [Paper](https://arxiv.org/abs/1502.03044), **"Meshed-Memory Transformer for Image Captioning"** [Paper](https://arxiv.org/abs/1905.01210), [Image Captioning](https://huggingface.co/docs/transformers/en/tasks/image_captioning)                                                                                                   | [Building an Image Captioning Model](link-to-notebook-11k)                                                                                                                                                                                                                                             |
| **Advancements in Multi-modal Capabilities** | **Visual Question Answering with VLMs: Bottom-Up and Top-Down Attention, Modular Networks** | [Visual Question Answering with Deep Learning](link-to-visual-question-answering-tutorial), **"Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering"** [Paper](https://arxiv.org/abs/1707.07998), [Visual Question Answering](https://huggingface.co/docs/transformers/en/tasks/visual_question_answering)                                                                              | [Building a Visual Question Answering Model](link-to-notebook-11l)                                                                                                                                                                                                                                         |
| **Multimodal LLMs: Emerging Trends** | **VILMA Benchmark for Spatio-Temporal Event Processing** | [ICLR 2024: VILMA Benchmark](link-to-vilma-benchmark), [VILMA: A Vision-Language Benchmark for Spatio-Temporal Event Understanding in Video](https://arxiv.org/abs/2307.13151)                                                                                                                                                                                                                             | [Evaluating Video-Language Models with VILMA](link-to-vilma-evaluation-notebook)                                                                                                                                                                                                      |
| **Multimodal LLMs: Emerging Trends** | **Generating Different Creative Text Formats,  like Poems, Code, Scripts, Musical Pieces, Email, Letters, etc., from a Single Prompt** | **"Language Models are Few-Shot Learners"** [Paper](https://arxiv.org/abs/2005.14165), [Generating Different Creative Text Formats](https://huggingface.co/blog/how-to-generate)                                                                                                                                                                                                                         | [Experimenting with Creative Text Formats Generation](link-to-creative-text-formats-notebook)                                                                                                                                                                                                      |


## 6.  Deploying and Managing NLP Systems

| Area                               | Topic                                                                    | Resources                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Practices                                                                                                                                                                                                                                                                          |
|------------------------------------|-------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Cloud Computing for NLP and LLMs** | **Deploying LLMs on Cloud Platforms (AWS, GCP, Azure): Understanding Different Services and Options** | [Deploying NLP Models on AWS](link-to-aws-nlp-deployment), [Deploying NLP Models on GCP](link-to-gcp-nlp-deployment), [Deploying NLP Models on Azure](link-to-azure-nlp-deployment), [Amazon SageMaker](https://aws.amazon.com/sagemaker/), [Google Cloud AI Platform](https://cloud.google.com/ai-platform), [Azure Machine Learning](https://azure.microsoft.com/en-us/services/machine-learning/)                                                                                                                | [Comparing and Choosing the Right Cloud Platform for LLM Deployment](link-to-cloud-platform-comparison-notebook)                                                                                                                                                                                |
| **Cloud Computing for NLP and LLMs** | **Optimizing LLM Inference on Cloud Platforms: Cost Efficiency, Latency Reduction** | [Optimizing Deep Learning Inference on AWS](https://aws.amazon.com/blogs/machine-learning/optimize-deep-learning-inference-on-aws/), [Optimizing Deep Learning Performance on GCP](https://cloud.google.com/ai-platform/training/docs/performance-tuning-tips), [Optimize inference performance](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-optimize-model-performance?tabs=python), **"A Survey on Efficient Inference for Large Language Models"** [Paper](https://arxiv.org/abs/2307.02230)     | [Optimizing LLM Inference for Cost and Performance](link-to-llm-inference-optimization-notebook)                                                                                                                                                                                |
| **Best Practices for LLM Deployment** | **MLOps Principles for LLM Deployment: Continuous Integration, Continuous Delivery, Monitoring** | [MLOps for NLP](link-to-mlops-for-nlp), [Practical MLOps](https://www.manning.com/books/practical-mlops), [MLOps: Continuous delivery and automation pipelines in machine learning](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)                                                                                                                                                       | [Implementing MLOps for LLM Deployment](link-to-notebook-12l)                                                                                                                                                                                |
| **Best Practices for LLM Deployment** | **Containerizing LLMs with Docker: Building and Managing LLM Containers** | [Docker for NLP](link-to-docker-for-nlp), [Docker Tutorial](https://docs.docker.com/get-started/), [Dockerizing a Machine Learning Model](https://towardsdatascience.com/dockerizing-a-machine-learning-model-57f85df6e341)                                                                                                                                                                                                                   | [Containerizing an LLM with Docker](link-to-notebook-12m)                                                                                                                                                                                |
| **Best Practices for LLM Deployment** | **Orchestrating LLM Deployments with Kubernetes: Scaling and Managing LLM Pods** | [Kubernetes for NLP](link-to-kubernetes-for-nlp), [Kubernetes Tutorial](https://kubernetes.io/docs/tutorials/kubernetes-basics/), [Deploying Machine Learning Models on Kubernetes](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)                                                                                                                                                                                     | [Orchestrating LLM Deployments with Kubernetes](link-to-notebook-12n)                                                                                                                                                                                |
| **Best Practices for LLM Deployment** | **Serverless Deployments for LLMs: Using AWS Lambda, Google Cloud Functions, Azure Functions** | [Serverless NLP](link-to-serverless-nlp), [Serverless Architectures](https://martinfowler.com/articles/serverless.html), [Serverless Machine Learning](https://aws.amazon.com/serverless/machine-learning/)                                                                                                                                                                                                                        | [Deploying an LLM as a Serverless Function](link-to-notebook-12o)                                                                                                                                                                                |
| **Best Practices for LLM Deployment** | **Building and Managing LLM APIs with FastAPI: Creating RESTful APIs for LLMs** | [FastAPI for NLP](link-to-fastapi-for-nlp), [FastAPI Tutorial](https://fastapi.tiangolo.com/tutorial/), [Building a Machine Learning API with FastAPI](https://towardsdatascience.com/building-a-machine-learning-api-with-fastapi-68d2cb09c142)                                                                                                                                                                                    | [Building and Managing an LLM API with FastAPI](link-to-notebook-12p)                                                                                                                                                                                |
| **LLM Deployment Platforms and Tools** | **Hugging Face Spaces for LLM Deployment: Simplifying LLM Deployment with a User-Friendly Interface** | [Hugging Face Spaces](link-to-huggingface-spaces), [Deploying Models with Hugging Face Spaces](https://huggingface.co/docs/hub/spaces-overview), [Gradio: Create UIs for your machine learning model in Python in 3 minutes](https://www.gradio.app/)                                                                                                                                                                                               | [Deploying an LLM with Hugging Face Spaces](link-to-notebook-12v)                                                                                                                                                                                |
| **LLM Deployment Platforms and Tools** | **BentoML for LLM Deployment: Model Packaging and Deployment Framework** | [BentoML](link-to-bentoml), [BentoML Documentation](https://docs.bentoml.org/), [BentoML: Build, Ship and Scale AI Applications](https://www.bentoml.com/)                                                                                                                                                                                                                                                                | [Deploying an LLM with BentoML](link-to-notebook-12w)                                                                                                                                                                                |
| **Open-Source LLMs and APIs**       | **Working with Open-Source LLMs (e.g., LLaMA, BLOOM): Understanding and Utilizing Open-Source Models** | [Open-Source LLMs](link-to-open-source-llms), **"LLaMA: Open and Efficient Foundation Language Models"** [Paper](https://arxiv.org/abs/2302.13971), [BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://bigscience.huggingface.co/blog/bloom)                                                                                                                                                                        | [Working with Open-Source LLMs](link-to-notebook-10s)                                                                                                                                                                                |
| **Open-Source LLMs and APIs**       | **Working with Commercial LLM APIs (OpenAI, Cohere, etc.): Integrating LLMs into Applications via APIs** | [Commercial LLM APIs](link-to-commercial-llm-apis), [OpenAI API Documentation](https://beta.openai.com/docs/introduction), [Cohere API Documentation](https://docs.cohere.ai/), [Anthropic API](https://www.anthropic.com/index/claude-now-available-in-anthropic-api)                                                                                                                                                                   | [Working with Commercial LLM APIs](link-to-notebook-10t)                                                                                                                                                                                |
| **Custom API Development for LLMs** | **Designing and Implementing LLM-Specific APIs: Building Custom APIs for Specific Use Cases** | [API Design for LLMs](link-to-api-design-llms), [REST API Design](https://restfulapi.net/), [Building APIs with FastAPI](https://fastapi.tiangolo.com/)                                                                                                                                                                                                                                                                           | [Building a Custom API for an LLM](link-to-custom-api-llms-notebook)                                                                                                                                                                                |
| **Monitoring and Maintaining LLM Performance** | **Tracking Key Metrics (Latency, Throughput, Error Rates)** | [Monitoring Machine Learning Models](https://www.jeremyjordan.me/monitoring-machine-learning-models/), [Monitoring LLM Performance](https://neptune.ai/blog/monitoring-llm-performance-metrics-and-tools), [WhyLabs: Observability Platform](https://whylabs.ai/)                                                                                                                                                                             | [Implementing Monitoring for Deployed LLMs](link-to-llm-monitoring-notebook)                                                                                                                                                                                |
| **Version Control and Model Management** | **Managing Different Versions of LLMs and Their Dependencies** | [DVC (Data Version Control)](https://dvc.org/), [MLflow](https://mlflow.org/), [Model Versioning](https://www.mlflow.org/docs/latest/concepts.html#model-versioning), [Managing Machine Learning Models with DVC](https://dvc.org/doc/use-cases/versioning-data-and-models/tutorial)                                                                                                                                                  | [Implementing Version Control and Model Management for LLMs](link-to-llm-version-control-notebook)                                                                                                                                                                                |


Good luck on your NLP journey!
